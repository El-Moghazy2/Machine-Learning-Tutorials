{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/El-Moghazy2/Machine-Learning-Tutorials/blob/master/Tutorials/building_rag_using_llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Custom-Chatbot using RAG\n",
        "\n",
        "In this tutorial we are going to explore how to use RAG to be able to use the reasoninig capabilities of open source LLMs with custom data without fine-tuning the model. We will make a RAG that answers your question based on one of HuggingFace courses that are publicly available.\n",
        "\n",
        "**Before** we dive into what Retrieval-Augmented Generation (RAG) is and why it’s useful, it's important to first understand the foundation it builds upon: Large Language Models (LLMs). Also known as foundational models, LLMs are transformer-based deep neural networks trained on massive volumes of publicly available text data. Like traditional language models, their primary objective is to predict the next word in a sequence—but on a much larger scale and with far more complexity.\n",
        "\n",
        "The term foundational reflects their broad training base: these models are designed to be general-purpose and adaptable across a wide range of tasks. Because of the extensive training data, LLMs are capable of generating human-like responses and offering reasonably useful general knowledge or limited reasoning but **their answers are limited to what they've seen during training.**\n",
        "\n",
        "However, this generality also brings limitations. Out of the box, LLMs are not ideal for tasks that **require domain-specific expertise** or **highly accurate answers** on the first try. They may also **\"hallucinate,\"** or generate plausible-sounding but **incorrect information**.\n",
        "\n",
        "To tailor these models for more specific applications, we often need to fine-tune them. Fine-tuning involves training the model further on labeled data specific to a target task or domain. For example, if we want the model to accurately classify emails as \"spam\" or \"not spam,\" we can fine-tune it using a dataset of emails paired with correct labels. This reduces hallucination and improves task-specific performance.\n",
        "\n",
        "The following figure illustrates the high-level process of pretraining and fine-tuning a foundational model."
      ],
      "metadata": {
        "id": "J9wSVSZfJIk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/El-Moghazy2/large-language-model-from-scratch/refs/heads/main/Figures/LLM.JPG?raw=true)"
      ],
      "metadata": {
        "id": "k0bgZ9lTJIk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While fine-tuning can significantly improve performance for specific applications, it comes with trade-offs. First, it requires collecting and curating a labeled dataset tailored to the task at hand which can be a time-consuming and expensive process. Additionally, the fine-tuning itself demands computational resources and expertise, which may not be readily available in all settings.\n",
        "\n",
        "Another important limitation is flexibility. If you need to update the model with new information to include recent documents or knowledge relevant to your chatbot, you'll likely need to go through the fine-tuning process again. This makes it a less convenient option for dynamic or frequently changing domains.\n",
        "\n",
        "This is precisely where Retrieval-Augmented Generation (RAG) becomes valuable."
      ],
      "metadata": {
        "id": "TbKlF3HBJIk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a RAG?\n",
        "\n",
        "As the name suggests, it has two main components: a Retrieval component and a Grounded Generation component.\n",
        "\n",
        "1. Retrieval: When you ask a question—say, \"What is data science?\"—the retrieval component first searches for and gathers all relevant chunks of text related to the question from a knowledge base, this knowledge base can be a company-related PDFs that are not publicly available.\n",
        "\n",
        "2. Grounded Generation: The generation process in a RAG system is \"grounded\" in the retrieved text, meaning the model can only generate answers based on that specific information. It doesn’t pull answers from outside the retrieved text, so you’re harnessing the reasoning and language abilities of the LLM to process and interpret the most relevant data, ensuring accurate and contextually appropriate responses.\n",
        "\n",
        "In essence, RAG systems combine fast, efficient data retrieval with the powerful reasoning capabilities of LLMs, making them ideal for generating accurate responses based on domain-specific or large datasets."
      ],
      "metadata": {
        "id": "QxOk7eAcJIk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/El-Moghazy2/large-language-model-from-scratch/refs/heads/main/Figures/new%20example.JPG?raw=true)"
      ],
      "metadata": {
        "id": "ub20TR6MJIk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are RAG systems used for:\n",
        "\n",
        "RAG systems are ideal for situations where a large language model (LLM) needs to answer questions based on specific data that the LLM wasn't trained on. For instance, if your company has a large repository of legal documents and you want to create a question-answering bot that can respond using that legal data, a RAG system is a perfect fit. **Here's why**:\n",
        "\n",
        "* Limited Training Data: Without fine-tuning, a general LLM won’t have access to the specific data (like your company's legal documents) since this data was not part of the model's original training set.\n",
        "\n",
        "* Avoiding Costly Fine-Tuning: Fine-tuning a large language model can be expensive and not necessary for this case. RAG offers a way to enhance the model's capabilities without the need for fine-tuning, by retrieving relevant data on the fly, without the need to finetune for each legal case you enter in the database.\n",
        "\n",
        "* Reducing Hallucinations: LLMs often \"hallucinate\" or generate plausible-sounding but incorrect information. By using a RAG system, you can provide the model with specific, contextually relevant information retrieved from your dataset, significantly reducing the likelihood of these hallucinations.\n",
        "\n",
        "**In essence, RAG allows you to leverage the power of LLMs while grounding their responses in the specific, up-to-date data relevant to your domain, without requiring costly model fine-tuning.**"
      ],
      "metadata": {
        "id": "tduN0bBJJIk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install chainlit\n",
        "!pip install sentence-transformers\n",
        "!pip install trulens-providers-litellm\n",
        "!pip install litellm\n",
        "!pip install faiss-gpu\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install langchain-huggingface\n",
        "!pip install openai\n",
        "!pip install -qU langchain-ollama\n",
        "!pip install langgraph\n",
        "!pip install ollama\n",
        "!pip install trulens trulens-apps-langchain\n",
        "!pip install langchain\n",
        "!pip install langchainhub\n",
        "!pip install trulens-providers-langchain\n",
        "!pip install packaging==21.3\n",
        "!pip install faiss-cpu bs4 tiktoken\n",
        "!pip install trulens-providers-huggingface\n",
        "!pip install --upgrade trulens pydantic\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install -U transformers\n",
        "!pip install hf_xet\n",
        "\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import kagglehub\n",
        "from uuid import uuid4\n",
        "from typing import Optional\n",
        "from typing_extensions import List, TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from IPython.display import display, clear_output, Image\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "!pip install ragas\n",
        "\n",
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.run_config import RunConfig\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
        "\n",
        "import requests\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from bs4 import BeautifulSoup\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from langchain import LLMChain, PromptTemplate, hub\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser, RetryOutputParser\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "tqdm.pandas(desc='DataFrame Operation')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-10T19:16:02.433494Z",
          "iopub.execute_input": "2025-04-10T19:16:02.433813Z"
        },
        "trusted": true,
        "scrolled": true,
        "id": "j86E7MEMJIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "trusted": true,
        "id": "_5AseOk4JIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import chainlit as cl\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: str):\n",
        "    # Your custom logic goes here...\n",
        "\n",
        "    # Send a response back to the user\n",
        "    await cl.Message(\n",
        "        content=f\"Received: {message}\",\n",
        "    ).send()"
      ],
      "metadata": {
        "id": "UAnq5Ltp8BJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"############\")\n",
        "print(\"IP v\")\n",
        "!curl https://ipv4.icanhazip.com/\n",
        "print(\"############\")"
      ],
      "metadata": {
        "id": "-xu323Qc8CS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit run app.py & npx localtunnel --port 8000"
      ],
      "metadata": {
        "id": "qDI2g5uf8G_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will start the Ollama server and download the model."
      ],
      "metadata": {
        "id": "aun_gCx9JIk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# I will use capture to remove the cell output, feel free to comment it to show all the progress bar\n",
        "# we can now start ollama using ollama serve command\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "subprocess.Popen(\"ollama serve\", shell=True)\n",
        "\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "t87mKlpVJIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -iTCP -sTCP:LISTEN -P | grep ollama"
      ],
      "metadata": {
        "trusted": true,
        "id": "1Y3Tzt8PJIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# now we pull/download the powerful llam3.1\n",
        "subprocess.Popen(\"ollama pull llama3.1:8b-instruct-q8_0\", shell=True)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q9_JXuSZJIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# now we pull/download the powerful llam3.3 we will use it later to test its embedding efect on the evaluation metrics\n",
        "subprocess.Popen(\"ollama pull llama3.3\", shell=True)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "TgJ63pmtRpBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also try to use other models from the Ollama library such as deepseek-r1 and llama3.3, but in this tutorial we will use llama3.1 instruct 8b"
      ],
      "metadata": {
        "id": "5-cPjSXkJIk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset:\n",
        "\n",
        "We aim to build a Retrieval-Augmented Generation (RAG) system using the Hugging Face NLP course. The RAG system will allow us to retrieve specific information from the course pages more efficiently by scraping the course content and organizing it in a way that the model can retrieve relevant information quickly.\n",
        "\n",
        "The course consists of many pages, each page of the follows a consistent URL pattern. The base URL remains the same, while the only changes occur in the chapter and page numbers. Here’s the general structure:\n",
        "\n",
        "For example:\n",
        "\n",
        "* Chapter 1, Page 1: https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "* Chapter 1, Page 2: https://huggingface.co/learn/nlp-course/chapter1/2\n",
        "- Chapter 2, Page 1: https://huggingface.co/learn/nlp-course/chapter2/1\n",
        "- Chapter 2, Page 2: https://huggingface.co/learn/nlp-course/chapter2/2"
      ],
      "metadata": {
        "id": "0LtKIkF7JIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "courses = []\n",
        "base_url = \"https://huggingface.co/learn/nlp-course/chapter{}/{}\"\n",
        "for i in range(12):\n",
        "    courses.extend(base_url.format(chapter, i) for chapter in range(20))\n",
        "\n",
        "def load_page(url):\n",
        "    \"\"\"Fetch the page content using requests.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "docs = []\n",
        "\n",
        "for url in courses:\n",
        "    html = load_page(url)\n",
        "    if html is None:\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    content_element = soup.select_one(\"div.prose-doc.prose.relative.mx-auto.max-w-4xl.break-words\")\n",
        "    if not content_element:\n",
        "        print(f\"Content element not found for {url}\")\n",
        "        continue\n",
        "\n",
        "    text_content = content_element.get_text(separator=\"\\n\").strip()\n",
        "\n",
        "    doc = Document(page_content=text_content, metadata={\"url\": url})\n",
        "    docs.append(doc)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(f\"Loaded {len(docs)} valid documents.\")"
      ],
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "ID4r8rDAJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step is to leverage Llama 3 open-source models. While there are various methods to do this, I will choose to host the model locally within a Kaggle environment. Using Ollama is one of the easiest and most convenient approaches for this setup."
      ],
      "metadata": {
        "id": "fyRxhfl4JIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOllama(\n",
        "    model='llama3.1:8b-instruct-q8_0',\n",
        "    temperature=0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wV0AoZMfJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since SentenceTransformer (SBERT) doesn't provide the exact interface that FAISS expects, we'll create a wrapper class around it to ensure seamless integration. This wrapper will adapt SBERT's functionality to match the API expected by FAISS, making them compatible for efficient vector search."
      ],
      "metadata": {
        "id": "8SxktCF-JIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub"
      ],
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "ODuc3ctOJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500,\n",
        "                                               add_start_index=True,\n",
        "                                               strip_whitespace=True,\n",
        "                                               separators=MARKDOWN_SEPARATORS)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "class SentenceTransformerWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.model.encode(text, convert_to_numpy=True).tolist()\n",
        "\n",
        "    def __call__(self, texts):\n",
        "        if isinstance(texts, list):\n",
        "            return self.embed_documents(texts)\n",
        "        else:\n",
        "            return self.embed_query(texts)\n",
        "\n",
        "embedding_model = SentenceTransformerWrapper(SentenceTransformer('all-MiniLM-L6-v2'))\n",
        "\n",
        "vector_store = FAISS.from_documents(splits, embedding_model)\n",
        "# only the most relevant 2 splits\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "trusted": true,
        "id": "orpTvg-hJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt + RAG\n",
        "\n",
        "LangChain provides predefined prompts for RAG setups, but I created a custom one tailored to the Llama 3 Instruct model. This model requires specific prompt formatting with system and user tags. You can find some useful examples in the [Llama3 prompt examples](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/)."
      ],
      "metadata": {
        "id": "90Edvm3VJIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "prompt_template_str = (\n",
        "    \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    You are a helpful AI course instructor and mentor that answers the user question based on the CONTEXT provided.\n",
        "    <|eot_id|>\n",
        "\n",
        "    <|start_header_id|>user<|end_header_id|>\n",
        "    Use the following pieces of retrieved CONTEXT to answer the question.\n",
        "\n",
        "    If the answer is not clear or not found in the CONTEXT, say:\n",
        "    **\"I don't know based on the provided context.\"**\n",
        "    DON'T answer based on your onw knowledge, don't answer if the QUESTION is not related to the CONTEXT\n",
        "\n",
        "    QUESTION:\n",
        "    {input}\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    <|eot_id|>\n",
        "\n",
        "    <|start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template_str,\n",
        "                            input_variables=['context', 'input'])\n",
        "\n",
        "\n",
        "def construct_rag_chain(llm, prompt, retriever):\n",
        "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "    # to simplify and inderstand how a chain looks like we can do it in one step below\n",
        "    rag_chain_only_str_answer = (\n",
        "        {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain, rag_chain_only_str_answer"
      ],
      "metadata": {
        "trusted": true,
        "id": "ezSVyOZXJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain, rag_chain_only_str_answer = construct_rag_chain(llm, prompt, retriever)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fl5W4FMxJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the output formatting, we'll create utility functions that generate consistent, well-structured, and visually clear prints for both the model's answers and supporting context documents."
      ],
      "metadata": {
        "id": "qaanT7-HJIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(query):\n",
        "\n",
        "    results = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "    answer = f\"\"\"Answer:\n",
        "    {results[\"answer\"]}\n",
        "    {\"=\" * 50}\n",
        "    \"\"\"\n",
        "\n",
        "    context = []\n",
        "    # Iterate over each document in the context and print its details in a neat format.\n",
        "    for idx, doc in enumerate(results[\"context\"], start=1):\n",
        "        source = doc.metadata.get(\"url\", \"N/A\")\n",
        "        text = re.sub(r'\\n+', '\\n', doc.page_content.strip())\n",
        "\n",
        "        context.append(f\"Document {idx}\")\n",
        "        context.append(f\"Source: {source}\")\n",
        "        context.append(\"-\" * 50)\n",
        "        context.append(\"Text:\")\n",
        "        context.append(text)\n",
        "        context.append(\"=\" * 50)\n",
        "\n",
        "    clear_output()\n",
        "    return answer, context\n",
        "\n",
        "def get_formatted_answer(answer, context):\n",
        "\n",
        "    print(answer)\n",
        "\n",
        "    if not \"I don't know\" in answer:\n",
        "        for line in context:\n",
        "            print(line)"
      ],
      "metadata": {
        "trusted": true,
        "id": "F76NGeosJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we ask about something in the course:"
      ],
      "metadata": {
        "id": "jiZJ8ydZJIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer, context = get_answer(\"What is a transformer?\")\n",
        "get_formatted_answer(answer, context)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hn1zJ62UJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "answer, context = get_answer(\"What are the transformer components?\")\n",
        "get_formatted_answer(answer, context)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nCGAEaMJJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And another questions that is irrelevant:"
      ],
      "metadata": {
        "id": "BVETCwSWJIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer, context = get_answer(\"What are cookies?\")\n",
        "get_formatted_answer(answer, context)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WEyzc0cwJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "There are various ways to build a Retrieval-Augmented Generation (RAG) system, typically involving three main stages: indexing, retrieval, and generation. Each stage offers multiple implementation options.  \n",
        "\n",
        "- **Indexing**: You can experiment with different vector store types, similarity measures, chunking strategies, and embedding models.  \n",
        "- **Generation**: You might choose between various LLM architectures or explore advanced setups like Agentic RAG, where multiple agents collaborate to generate a better response.  \n",
        "\n",
        "However, to determine which combination works best for your specific use case, a solid evaluation strategy is essential.\n",
        "\n",
        "## Evaluation Approaches\n",
        "\n",
        "There are several ways to evaluate the performance of a RAG system:\n",
        "\n",
        "- **Human Evaluation**: The most reliable method involves using human-labeled datasets for benchmarking. While this provides high-quality feedback, it's often impractical for early-stage projects or quick proofs of concept due to time and cost constraints.\n",
        "\n",
        "- **LLM-as-a-Judge**: A more scalable, though still evolving, approach is to use large language models to evaluate the quality of responses. This method is gaining popularity, especially in rapid prototyping scenarios.\n",
        "\n",
        "In this tutorial, we’ll focus on automatic evaluation techniques based on key quality metrics, using two popular libraries:\n",
        "\n",
        "- [**TruLens**](https://www.trulens.org/)\n",
        "- [**Ragas**](https://github.com/explodinggradients/ragas)\n",
        "\n",
        "These tools help us assess critical aspects of RAG performance such as faithfulness, context relevance, and answer relevance.\n",
        "\n",
        "\n",
        "\n",
        "## TruLens\n",
        "\n",
        "TruLens offers three core metrics to evaluate a RAG system:\n",
        "\n",
        "- **Context Relevance**: Assesses whether the retrieved context is actually relevant to the user's query, helping reduce irrelevant or distracting information.\n",
        "  \n",
        "- **Groundedness**: Measures how well the generated answer sticks to the retrieved context, ensuring the model doesn’t \"hallucinate\" unsupported facts.\n",
        "  \n",
        "- **Answer Relevance**: Evaluates whether the final response meaningfully addresses the user's original query, focusing on usefulness and clarity."
      ],
      "metadata": {
        "id": "zu3LSvhmJIk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.trulens.org/getting_started/core_concepts/rag_triad/\n",
        "\n",
        "Note that the Trulens chain requires the rag to output only the text answer so we are going to use the chain that has only text values."
      ],
      "metadata": {
        "id": "7o6VeuK2JIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.apps.langchain import TruChain\n",
        "from trulens.core import TruSession\n",
        "\n",
        "session = TruSession()\n",
        "session.reset_database()"
      ],
      "metadata": {
        "trusted": true,
        "id": "eWovRXGPJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "from trulens.providers.litellm import LiteLLM\n",
        "from trulens.core import Feedback\n",
        "\n",
        "litellm.set_verbose = False\n",
        "\n",
        "# first we need to load the LLM judge fromm ollama\n",
        "ollama_provider = LiteLLM(\n",
        "    model_engine=\"ollama/llama3.1:8b-instruct-q8_0\", api_base=\"http://localhost:11434\"\n",
        ")\n",
        "\n",
        "context = TruChain.select_context(rag_chain_only_str_answer)\n",
        "\n",
        "# now we define our metrics, and the stage of evaluation, for example the groundness will be checked after\n",
        "# after answer generation (getting the answer) and after context generation (getting the context)\n",
        "f_groundedness = (\n",
        "    Feedback(\n",
        "        ollama_provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
        "    )\n",
        "    .on(context.collect())  # collect context chunks into a list\n",
        "    .on_output()\n",
        ")\n",
        "\n",
        "f_answer_relevance = Feedback(\n",
        "    ollama_provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
        ").on_input_output()\n",
        "\n",
        "f_context_relevance = (\n",
        "    Feedback(\n",
        "        ollama_provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
        "    )\n",
        "    .on_input()\n",
        "    .on(context)\n",
        "    .aggregate(np.mean)\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "f3vj4Ke2JIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.apps.app import TruApp\n",
        "\n",
        "tru_recorder = TruChain(\n",
        "    rag_chain_only_str_answer,\n",
        "    app_name=\"ChatApplication\",\n",
        "    app_version=\"Chain1\",\n",
        "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
        ")\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "trusted": true,
        "id": "SlpHCI7cJIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's have some questions that are related to the course that we scrapped that the rag can answer to calculate the metrics."
      ],
      "metadata": {
        "id": "7gSzXrEFJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_lst_tru = [\"What is a transformers?\",\n",
        "         \"What are the transformer components?\",\n",
        "         \"What is the Encoder?\",\n",
        "         \"Explain the transformer work theory\",\n",
        "         \"What is self-Attention\",\n",
        "         \"What is the bias of transformers?\",\n",
        "         \"What are transformer limits?\"\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "id": "TW8ftwH_JIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "final step is to record the metrics as we invoke the model on each question"
      ],
      "metadata": {
        "id": "IbATGtLuJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tru_recorder as recording:\n",
        "    for question in q_lst_tru:\n",
        "        llm_response = rag_chain.invoke({\"input\": question})[\"answer\"]\n",
        "        print(llm_response)\n",
        "        print(\"=\" * 50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "C0tnG54_JIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can check the dashboard below."
      ],
      "metadata": {
        "id": "0fi-flyaJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session.get_leaderboard()"
      ],
      "metadata": {
        "trusted": true,
        "id": "F7ulawAOJIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the answers are not bad in many cases, the context relevance can still be improved but the groundness is the best so far."
      ],
      "metadata": {
        "id": "sSFFBb7HJIk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ragas\n",
        "\n",
        "Another valuable library for evaluating LLM-based RAG systems is **Ragas**. It enables you to assess the quality of your system by comparing its outputs against **expected (reference) answers**. These reference answers are typically written by humans, but Ragas also supports generating them using an LLM — a feature we won’t use in this tutorial to maintain control over the evaluation process.\n",
        "\n",
        "> _Note: The reference answers used here were generated using GitHub Copilot, based on the same course materials that the RAG system uses._\n",
        "\n",
        "Ragas uses a separate LLM to act as a judge, scoring your system’s responses across several key dimensions, such as faithfulness and relevance. This approach allows for a more nuanced and automated evaluation compared to traditional manual reviews."
      ],
      "metadata": {
        "id": "otvmj0bpJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_lst = [\"What are transformers?\",\n",
        "         \"What are the transformer components?\",\n",
        "         \"What is the Encoder?\"\n",
        "]\n",
        "\n",
        "expected_responses = [\n",
        "    \"Transformers are powerful deep learning models designed for various natural language processing tasks. They can perform sentiment analysis, text generation, summarization, translation, and even answer questions based on context. Hugging Face provides a vast collection of pretrained Transformer models that can be easily accessed using the pipeline() function.\",\n",
        "    \"Transformers consist of two main components: the encoder, which processes and understands the input, and the decoder, which generates the output based on the encoder's representation. A key feature of transformers is the attention layers, which allow the model to focus on relevant parts of the input for better performance. These components work together to handle tasks like translation, summarization, and text generation.\",\n",
        "    \"The encoder is a key component of transformer models, designed to process and build a meaningful representation of the input data (like text). It extracts features from the input, which are then optimized for understanding and interpretation. This representation is later used by the decoder or directly for tasks like classification and named entity recognition.\",\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "id": "aqqRUBK_JIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_eval_dataset(q_lst, expected_responses, chain):\n",
        "  dataset = []\n",
        "\n",
        "  for query, reference in zip(q_lst, expected_responses):\n",
        "      relevant_docs = rag_chain.invoke({\"input\": query})[\"context\"]\n",
        "      response = rag_chain.invoke({\"input\": query})[\"answer\"]\n",
        "      dataset.append(\n",
        "          {\n",
        "              \"user_input\": query,\n",
        "              \"retrieved_contexts\": [rdoc.page_content for rdoc in relevant_docs],\n",
        "              \"response\": response,\n",
        "              \"reference\": reference,\n",
        "          }\n",
        "      )\n",
        "\n",
        "  evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
        "\n",
        "  return evaluation_dataset, dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "bWnf8MVVJIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset, dataset = construct_eval_dataset(q_lst, expected_responses, rag_chain)"
      ],
      "metadata": {
        "id": "40LugXzmoR2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataset:\n",
        "  print(data['user_input'])\n",
        "  print(data['response'])\n",
        "  print(data['reference'])\n",
        "  print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "H7hJUHKtCcYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_config = RunConfig(timeout=5000)"
      ],
      "metadata": {
        "trusted": true,
        "id": "796BO_1XJIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_llm = LangchainLLMWrapper(llm, run_config)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
        "    llm=evaluator_llm,\n",
        ")\n",
        "\n",
        "result"
      ],
      "metadata": {
        "trusted": true,
        "id": "zw7gsCzEJIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how to evaluate it, let's try again but this time with a different, more powerful embedding model."
      ],
      "metadata": {
        "id": "5pdT5-aEJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = ChatOllama(\n",
        "    model='llama3.3',\n",
        "    temperature=0)\n",
        "\n",
        "vector_store = FAISS.from_documents(splits, embedding_model)\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "rag_chain_improved, rag_chain_only_str_answer = construct_rag_chain(llm, prompt, retriever)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yqJpZfhgJIk5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset, dataset = construct_eval_dataset(q_lst, expected_responses, rag_chain)\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(llm, run_config)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
        "    llm=evaluator_llm,\n",
        ")\n",
        "\n",
        "result"
      ],
      "metadata": {
        "trusted": true,
        "id": "lAZKczZxJIk5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with tru_recorder as recording:\n",
        "    for question in q_lst_tru:\n",
        "        llm_response = rag_chain_improved.invoke({\"input\": question})[\"answer\"]\n",
        "        print(llm_response)\n",
        "        print(\"=\" * 50)"
      ],
      "metadata": {
        "id": "3G_HNKCgrKqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session.get_leaderboard()"
      ],
      "metadata": {
        "id": "FOKXj1xnyJD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we explored the integration of a Retrieval-Augmented Generation (RAG) system using Llama 3.1. By combining the generative strengths of Llama with a simple retrieval process, we aimed to enhance the model’s output with relevant and accurate context to improve the model output and make it more reliable. Here are the key takeaways:\n",
        "\n",
        "- **Enhanced Relevance:** Incorporating real-time data retrieval into the generation process improves the accuracy and contextual relevance of the output, reduces halucinations of foundationa models and makes us confident about the sosurces of the data without relying on the training data of the LLM itself.\n",
        "- **Room for Optimization:** While the initial results are promising, future work could focus on refining retrieval strategies and fine-tuning the prompt-engineering techniques to further boost performance. Experimenting with alternative datasets and optimizing query algorithms can also pave the way for more robust implementations.\n",
        "\n",
        "Overall, this notebook demonstrates a constructive step towards building AI systems that are not only generative but also context-aware and fact-guided. The insights gained here is the basic foundation on which you can build to improve your RAG application."
      ],
      "metadata": {
        "id": "CLQMq75eJIk5"
      }
    }
  ]
}