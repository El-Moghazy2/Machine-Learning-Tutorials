{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e522f1fb0a5e4d7b99c4cbc37c5f8e75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8533668f8c7d4741af87613b2ca5a096",
              "IPY_MODEL_2e6c8fff1f33429194df9877dc7471dc",
              "IPY_MODEL_2afb2edbace646c980c23f3c6db0538a"
            ],
            "layout": "IPY_MODEL_e79eab4767184a669e17524454aa5e6d"
          }
        },
        "8533668f8c7d4741af87613b2ca5a096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d80fd5c245fd44c9a4c23ea9ea61c852",
            "placeholder": "​",
            "style": "IPY_MODEL_64f507aaf3174d1e9ed71df01d134a78",
            "value": "Evaluating: 100%"
          }
        },
        "2e6c8fff1f33429194df9877dc7471dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cc74c844efe4c20bac50550fb2d50a2",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_491d816f53334947ba2c9ffdc4dda402",
            "value": 9
          }
        },
        "2afb2edbace646c980c23f3c6db0538a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b319c7c6288c4823b6d321417d115bcf",
            "placeholder": "​",
            "style": "IPY_MODEL_3b9612fedf474dfeb17ccb417970a8db",
            "value": " 9/9 [02:07&lt;00:00,  9.94s/it]"
          }
        },
        "e79eab4767184a669e17524454aa5e6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80fd5c245fd44c9a4c23ea9ea61c852": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64f507aaf3174d1e9ed71df01d134a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cc74c844efe4c20bac50550fb2d50a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "491d816f53334947ba2c9ffdc4dda402": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b319c7c6288c4823b6d321417d115bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b9612fedf474dfeb17ccb417970a8db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "375e0ce03699491699c301de71838ab3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_00672e8026aa441e8e5f7e48d23beb60",
              "IPY_MODEL_498ea67d175149708c16a4364da4a307",
              "IPY_MODEL_02f443a38c984c46bab4d7796e0a47fe"
            ],
            "layout": "IPY_MODEL_042070bc87b64e0cbf495ea3044fa00b"
          }
        },
        "00672e8026aa441e8e5f7e48d23beb60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bc3b0ca28914bd782264ddb2e37eee4",
            "placeholder": "​",
            "style": "IPY_MODEL_525bed5711f544a8b83fed41a3647b89",
            "value": "Evaluating: 100%"
          }
        },
        "498ea67d175149708c16a4364da4a307": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a5445d252904faba185e3cbb2ac6c24",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d7f6bd6568f4b3ebf993b99dbd2e306",
            "value": 9
          }
        },
        "02f443a38c984c46bab4d7796e0a47fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a37a10618f1b47b89a48d069569fc19a",
            "placeholder": "​",
            "style": "IPY_MODEL_52eb81afa655453cae56c75c2010ea40",
            "value": " 9/9 [02:09&lt;00:00,  8.33s/it]"
          }
        },
        "042070bc87b64e0cbf495ea3044fa00b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bc3b0ca28914bd782264ddb2e37eee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "525bed5711f544a8b83fed41a3647b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a5445d252904faba185e3cbb2ac6c24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d7f6bd6568f4b3ebf993b99dbd2e306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a37a10618f1b47b89a48d069569fc19a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52eb81afa655453cae56c75c2010ea40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/El-Moghazy2/Machine-Learning-Tutorials/blob/master/building_rag_using_llama3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Custom-Chatbot using RAG\n",
        "\n",
        "In this tutorial we are going to explore how to use RAG to be able to use the reasoninig capabilities of open source LLMs with custom data without fine-tuning the model. We will make a RAG that answers your question based on one of HuggingFace courses that are publicly available.\n",
        "\n",
        "**Before** we dive into what Retrieval-Augmented Generation (RAG) is and why it’s useful, it's important to first understand the foundation it builds upon: Large Language Models (LLMs). Also known as foundational models, LLMs are transformer-based deep neural networks trained on massive volumes of publicly available text data. Like traditional language models, their primary objective is to predict the next word in a sequence—but on a much larger scale and with far more complexity.\n",
        "\n",
        "The term foundational reflects their broad training base: these models are designed to be general-purpose and adaptable across a wide range of tasks. Because of the extensive training data, LLMs are capable of generating human-like responses and offering reasonably useful general knowledge or limited reasoning but **their answers are limited to what they've seen during training.**\n",
        "\n",
        "However, this generality also brings limitations. Out of the box, LLMs are not ideal for tasks that **require domain-specific expertise** or **highly accurate answers** on the first try. They may also **\"hallucinate,\"** or generate plausible-sounding but **incorrect information**.\n",
        "\n",
        "To tailor these models for more specific applications, we often need to fine-tune them. Fine-tuning involves training the model further on labeled data specific to a target task or domain. For example, if we want the model to accurately classify emails as \"spam\" or \"not spam,\" we can fine-tune it using a dataset of emails paired with correct labels. This reduces hallucination and improves task-specific performance.\n",
        "\n",
        "The following figure illustrates the high-level process of pretraining and fine-tuning a foundational model."
      ],
      "metadata": {
        "id": "J9wSVSZfJIk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/El-Moghazy2/large-language-model-from-scratch/refs/heads/main/Figures/LLM.JPG?raw=true)"
      ],
      "metadata": {
        "id": "k0bgZ9lTJIk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While fine-tuning can significantly improve performance for specific applications, it comes with trade-offs. First, it requires collecting and curating a labeled dataset tailored to the task at hand which can be a time-consuming and expensive process. Additionally, the fine-tuning itself demands computational resources and expertise, which may not be readily available in all settings.\n",
        "\n",
        "Another important limitation is flexibility. If you need to update the model with new information to include recent documents or knowledge relevant to your chatbot, you'll likely need to go through the fine-tuning process again. This makes it a less convenient option for dynamic or frequently changing domains.\n",
        "\n",
        "This is precisely where Retrieval-Augmented Generation (RAG) becomes valuable."
      ],
      "metadata": {
        "id": "TbKlF3HBJIk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is a RAG?\n",
        "\n",
        "As the name suggests, it has two main components: a Retrieval component and a Grounded Generation component.\n",
        "\n",
        "1. Retrieval: When you ask a question—say, \"What is data science?\"—the retrieval component first searches for and gathers all relevant chunks of text related to the question from a knowledge base, this knowledge base can be a company-related PDFs that are not publicly available.\n",
        "\n",
        "2. Grounded Generation: The generation process in a RAG system is \"grounded\" in the retrieved text, meaning the model can only generate answers based on that specific information. It doesn’t pull answers from outside the retrieved text, so you’re harnessing the reasoning and language abilities of the LLM to process and interpret the most relevant data, ensuring accurate and contextually appropriate responses.\n",
        "\n",
        "In essence, RAG systems combine fast, efficient data retrieval with the powerful reasoning capabilities of LLMs, making them ideal for generating accurate responses based on domain-specific or large datasets."
      ],
      "metadata": {
        "id": "QxOk7eAcJIk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://raw.githubusercontent.com/El-Moghazy2/large-language-model-from-scratch/refs/heads/main/Figures/new%20example.JPG?raw=true)"
      ],
      "metadata": {
        "id": "ub20TR6MJIk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are RAG systems used for:\n",
        "\n",
        "RAG systems are ideal for situations where a large language model (LLM) needs to answer questions based on specific data that the LLM wasn't trained on. For instance, if your company has a large repository of legal documents and you want to create a question-answering bot that can respond using that legal data, a RAG system is a perfect fit. **Here's why**:\n",
        "\n",
        "* Limited Training Data: Without fine-tuning, a general LLM won’t have access to the specific data (like your company's legal documents) since this data was not part of the model's original training set.\n",
        "\n",
        "* Avoiding Costly Fine-Tuning: Fine-tuning a large language model can be expensive and not necessary for this case. RAG offers a way to enhance the model's capabilities without the need for fine-tuning, by retrieving relevant data on the fly, without the need to finetune for each legal case you enter in the database.\n",
        "\n",
        "* Reducing Hallucinations: LLMs often \"hallucinate\" or generate plausible-sounding but incorrect information. By using a RAG system, you can provide the model with specific, contextually relevant information retrieved from your dataset, significantly reducing the likelihood of these hallucinations.\n",
        "\n",
        "**In essence, RAG allows you to leverage the power of LLMs while grounding their responses in the specific, up-to-date data relevant to your domain, without requiring costly model fine-tuning.**"
      ],
      "metadata": {
        "id": "tduN0bBJJIk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install chainlit\n",
        "!pip install sentence-transformers\n",
        "!pip install trulens-providers-litellm\n",
        "!pip install litellm\n",
        "!pip install faiss-gpu\n",
        "!pip install langchain\n",
        "!pip install langchain_community\n",
        "!pip install langchain-huggingface\n",
        "!pip install openai\n",
        "!pip install -qU langchain-ollama\n",
        "!pip install langgraph\n",
        "!pip install ollama\n",
        "!pip install trulens trulens-apps-langchain\n",
        "!pip install langchain\n",
        "!pip install langchainhub\n",
        "!pip install trulens-providers-langchain\n",
        "!pip install packaging==21.3\n",
        "!pip install faiss-cpu bs4 tiktoken\n",
        "!pip install trulens-providers-huggingface\n",
        "!pip install --upgrade trulens pydantic\n",
        "!pip install --upgrade scikit-learn\n",
        "!pip install -U transformers\n",
        "!pip install hf_xet\n",
        "\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import kagglehub\n",
        "from uuid import uuid4\n",
        "from typing import Optional\n",
        "from typing_extensions import List, TypedDict\n",
        "from pydantic import BaseModel, Field\n",
        "from IPython.display import display, clear_output, Image\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "!pip install ragas\n",
        "\n",
        "from ragas import EvaluationDataset\n",
        "from ragas import evaluate\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.run_config import RunConfig\n",
        "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness\n",
        "\n",
        "import requests\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from bs4 import BeautifulSoup\n",
        "from langgraph.graph import START, StateGraph\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from langchain import LLMChain, PromptTemplate, hub\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser, RetryOutputParser\n",
        "from langchain_core.output_parsers.string import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.documents import Document\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "tqdm.pandas(desc='DataFrame Operation')\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2025-04-10T19:16:02.433494Z",
          "iopub.execute_input": "2025-04-10T19:16:02.433813Z"
        },
        "trusted": true,
        "scrolled": true,
        "id": "j86E7MEMJIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "trusted": true,
        "id": "_5AseOk4JIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import chainlit as cl\n",
        "\n",
        "@cl.on_message\n",
        "async def main(message: str):\n",
        "    # Your custom logic goes here...\n",
        "\n",
        "    # Send a response back to the user\n",
        "    await cl.Message(\n",
        "        content=f\"Received: {message}\",\n",
        "    ).send()"
      ],
      "metadata": {
        "id": "UAnq5Ltp8BJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"############\")\n",
        "print(\"IP v\")\n",
        "!curl https://ipv4.icanhazip.com/\n",
        "print(\"############\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xu323Qc8CS7",
        "outputId": "ccc4bd20-11e9-48e2-8e51-5e40c43db8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "############\n",
            "IP v\n",
            "34.132.228.56\n",
            "############\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit run app.py & npx localtunnel --port 8000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDI2g5uf8G_7",
        "outputId": "de94adf0-064e-4492-87e8-3e921a8e8d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0Kyour url is: https://tired-tigers-deny.loca.lt\n",
            "Usage: chainlit run [OPTIONS] TARGET\n",
            "Try 'chainlit run --help' for help.\n",
            "\n",
            "Error: Invalid value: File does not exist: app.py\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will start the Ollama server and download the model."
      ],
      "metadata": {
        "id": "aun_gCx9JIk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# I will use capture to remove the cell output, feel free to comment it to show all the progress bar\n",
        "# we can now start ollama using ollama serve command\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "subprocess.Popen(\"ollama serve\", shell=True)\n",
        "\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "scrolled": true,
        "trusted": true,
        "id": "t87mKlpVJIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -iTCP -sTCP:LISTEN -P | grep ollama"
      ],
      "metadata": {
        "trusted": true,
        "id": "1Y3Tzt8PJIk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b82f3be-e551-4260-8630-8cfba9519ba5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ollama    14467 root    3u  IPv4 381306      0t0  TCP localhost:11434 (LISTEN)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# now we pull/download the powerful llam3.1\n",
        "subprocess.Popen(\"ollama pull llama3.1:8b-instruct-q8_0\", shell=True)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Q9_JXuSZJIk2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# now we pull/download the powerful llam3.3 we will use it later to test its embedding efect on the evaluation metrics\n",
        "subprocess.Popen(\"ollama pull llama3.3\", shell=True)\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "TgJ63pmtRpBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also try to use other models from the Ollama library such as deepseek-r1 and llama3.3, but in this tutorial we will use llama3.1 instruct 8b"
      ],
      "metadata": {
        "id": "5-cPjSXkJIk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset:\n",
        "\n",
        "We aim to build a Retrieval-Augmented Generation (RAG) system using the Hugging Face NLP course. The RAG system will allow us to retrieve specific information from the course pages more efficiently by scraping the course content and organizing it in a way that the model can retrieve relevant information quickly.\n",
        "\n",
        "The course consists of many pages, each page of the follows a consistent URL pattern. The base URL remains the same, while the only changes occur in the chapter and page numbers. Here’s the general structure:\n",
        "\n",
        "For example:\n",
        "\n",
        "* Chapter 1, Page 1: https://huggingface.co/learn/nlp-course/chapter1/1\n",
        "* Chapter 1, Page 2: https://huggingface.co/learn/nlp-course/chapter1/2\n",
        "- Chapter 2, Page 1: https://huggingface.co/learn/nlp-course/chapter2/1\n",
        "- Chapter 2, Page 2: https://huggingface.co/learn/nlp-course/chapter2/2"
      ],
      "metadata": {
        "id": "0LtKIkF7JIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "courses = []\n",
        "base_url = \"https://huggingface.co/learn/nlp-course/chapter{}/{}\"\n",
        "for i in range(12):\n",
        "    courses.extend(base_url.format(chapter, i) for chapter in range(20))\n",
        "\n",
        "def load_page(url):\n",
        "    \"\"\"Fetch the page content using requests.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        if response.status_code == 200:\n",
        "            return response.text\n",
        "        else:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {url}: {e}\")\n",
        "    return None\n",
        "\n",
        "docs = []\n",
        "\n",
        "for url in courses:\n",
        "    html = load_page(url)\n",
        "    if html is None:\n",
        "        continue\n",
        "\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    content_element = soup.select_one(\"div.prose-doc.prose.relative.mx-auto.max-w-4xl.break-words\")\n",
        "    if not content_element:\n",
        "        print(f\"Content element not found for {url}\")\n",
        "        continue\n",
        "\n",
        "    text_content = content_element.get_text(separator=\"\\n\").strip()\n",
        "\n",
        "    doc = Document(page_content=text_content, metadata={\"url\": url})\n",
        "    docs.append(doc)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "print(f\"Loaded {len(docs)} valid documents.\")"
      ],
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "ID4r8rDAJIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4e2e1a8-7749-4d57-93b9-5e5a2c6ab0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 95 valid documents.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our next step is to leverage Llama 3 open-source models. While there are various methods to do this, I will choose to host the model locally within a Kaggle environment. Using Ollama is one of the easiest and most convenient approaches for this setup."
      ],
      "metadata": {
        "id": "fyRxhfl4JIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOllama(\n",
        "    model='llama3.1:8b-instruct-q8_0',\n",
        "    temperature=0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "wV0AoZMfJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since SentenceTransformer (SBERT) doesn't provide the exact interface that FAISS expects, we'll create a wrapper class around it to ensure seamless integration. This wrapper will adapt SBERT's functionality to match the API expected by FAISS, making them compatible for efficient vector search."
      ],
      "metadata": {
        "id": "8SxktCF-JIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub"
      ],
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "id": "ODuc3ctOJIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d47120-f4b9-4cd2-b005-ba3687068ea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "MARKDOWN_SEPARATORS = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\",\n",
        "]\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=500,\n",
        "                                               add_start_index=True,\n",
        "                                               strip_whitespace=True,\n",
        "                                               separators=MARKDOWN_SEPARATORS)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "\n",
        "class SentenceTransformerWrapper:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def embed_documents(self, texts):\n",
        "        return self.model.encode(texts, convert_to_numpy=True).tolist()\n",
        "\n",
        "    def embed_query(self, text):\n",
        "        return self.model.encode(text, convert_to_numpy=True).tolist()\n",
        "\n",
        "    def __call__(self, texts):\n",
        "        if isinstance(texts, list):\n",
        "            return self.embed_documents(texts)\n",
        "        else:\n",
        "            return self.embed_query(texts)\n",
        "\n",
        "embedding_model = SentenceTransformerWrapper(SentenceTransformer('all-MiniLM-L6-v2'))\n",
        "\n",
        "vector_store = FAISS.from_documents(splits, embedding_model)\n",
        "# only the most relevant 2 splits\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "trusted": true,
        "id": "orpTvg-hJIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1f74aa2-6c07-4541-9126-87084331fa10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.vectorstores.faiss:`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt + RAG\n",
        "\n",
        "LangChain provides predefined prompts for RAG setups, but I created a custom one tailored to the Llama 3 Instruct model. This model requires specific prompt formatting with system and user tags. You can find some useful examples in the [Llama3 prompt examples](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/)."
      ],
      "metadata": {
        "id": "90Edvm3VJIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "prompt_template_str = (\n",
        "    \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "    You are a helpful AI course instructor and mentor that answers the user question based on the CONTEXT provided.\n",
        "    <|eot_id|>\n",
        "\n",
        "    <|start_header_id|>user<|end_header_id|>\n",
        "    Use the following pieces of retrieved CONTEXT to answer the question.\n",
        "\n",
        "    If the answer is not clear or not found in the CONTEXT, say:\n",
        "    **\"I don't know based on the provided context.\"**\n",
        "    DON'T answer based on your onw knowledge, don't answer if the QUESTION is not related to the CONTEXT\n",
        "\n",
        "    QUESTION:\n",
        "    {input}\n",
        "\n",
        "    CONTEXT:\n",
        "    {context}\n",
        "\n",
        "    <|eot_id|>\n",
        "\n",
        "    <|start_header_id|>assistant<|end_header_id|>\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "prompt = PromptTemplate(template=prompt_template_str,\n",
        "                            input_variables=['context', 'input'])\n",
        "\n",
        "\n",
        "def construct_rag_chain(llm, prompt, retriever):\n",
        "    question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n",
        "\n",
        "    # to simplify and inderstand how a chain looks like we can do it in one step below\n",
        "    rag_chain_only_str_answer = (\n",
        "        {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "\n",
        "    return rag_chain, rag_chain_only_str_answer"
      ],
      "metadata": {
        "trusted": true,
        "id": "ezSVyOZXJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain, rag_chain_only_str_answer = construct_rag_chain(llm, prompt, retriever)"
      ],
      "metadata": {
        "trusted": true,
        "id": "fl5W4FMxJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "To improve the output formatting, we'll create utility functions that generate consistent, well-structured, and visually clear prints for both the model's answers and supporting context documents."
      ],
      "metadata": {
        "id": "qaanT7-HJIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(query):\n",
        "\n",
        "    results = rag_chain.invoke({\"input\": query})\n",
        "\n",
        "    answer = f\"\"\"Answer:\n",
        "    {results[\"answer\"]}\n",
        "    {\"=\" * 50}\n",
        "    \"\"\"\n",
        "\n",
        "    context = []\n",
        "    # Iterate over each document in the context and print its details in a neat format.\n",
        "    for idx, doc in enumerate(results[\"context\"], start=1):\n",
        "        source = doc.metadata.get(\"url\", \"N/A\")\n",
        "        text = re.sub(r'\\n+', '\\n', doc.page_content.strip())\n",
        "\n",
        "        context.append(f\"Document {idx}\")\n",
        "        context.append(f\"Source: {source}\")\n",
        "        context.append(\"-\" * 50)\n",
        "        context.append(\"Text:\")\n",
        "        context.append(text)\n",
        "        context.append(\"=\" * 50)\n",
        "\n",
        "    clear_output()\n",
        "    return answer, context\n",
        "\n",
        "def get_formatted_answer(answer, context):\n",
        "\n",
        "    print(answer)\n",
        "\n",
        "    if not \"I don't know\" in answer:\n",
        "        for line in context:\n",
        "            print(line)"
      ],
      "metadata": {
        "trusted": true,
        "id": "F76NGeosJIk3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we ask about something in the course:"
      ],
      "metadata": {
        "id": "jiZJ8ydZJIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer, context = get_answer(\"What is a transformer?\")\n",
        "get_formatted_answer(answer, context)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hn1zJ62UJIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a82c0a69-a410-41a1-cd21-e342b680ffda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "    According to the context, a transformer is a type of model used for Natural Language Processing (NLP) tasks. It's primarily composed of two blocks: an Encoder and a Decoder. The Encoder builds a representation of the input, while the Decoder uses this representation along with other inputs to generate a target sequence.\n",
            "\n",
            "Additionally, Transformer models are built with special layers called attention layers, which allow them to focus on specific parts of the input when generating output.\n",
            "\n",
            "The context also mentions that Transformers can be used for various NLP tasks, such as sentence classification, named entity recognition, text generation, translation, and summarization.\n",
            "    ==================================================\n",
            "    \n",
            "Document 1\n",
            "Source: https://huggingface.co/learn/nlp-course/chapter1/3\n",
            "--------------------------------------------------\n",
            "Text:\n",
            "Transformers, what can they do?\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "In this section, we will look at what Transformer models can do and use our first tool from the 🤗 Transformers library: the \n",
            "pipeline()\n",
            " function.\n",
            " \n",
            "👀 See that \n",
            "Open in Colab\n",
            " button on the top right? Click on it to open a Google Colab notebook with all the code samples of this section. This button will be present in any section containing code examples. \n",
            "If you want to run the examples locally, we recommend taking a look at the \n",
            "setup\n",
            ".\n",
            " \n",
            " \n",
            "Transformers are everywhere!\n",
            " \n",
            "Transformer models are used to solve all kinds of NLP tasks, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:\n",
            " \n",
            " \n",
            "The \n",
            "🤗 Transformers library\n",
            " provides the functionality to create and use those shared models. The \n",
            "Model Hub\n",
            " contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub!\n",
            " \n",
            "⚠️ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! \n",
            "Create a huggingface.co\n",
            " account to benefit from all available features!\n",
            " \n",
            "Before diving into how Transformer models work under the hood, let’s look at a few examples of how they can be used to solve some interesting NLP problems.\n",
            " \n",
            " \n",
            "Working with pipelines\n",
            " \n",
            " \n",
            "The most basic object in the 🤗 Transformers library is the \n",
            "pipeline()\n",
            " function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n",
            " \n",
            " \n",
            " Copied\n",
            " \n",
            "from\n",
            " transformers \n",
            "import\n",
            " pipeline\n",
            "==================================================\n",
            "Document 2\n",
            "Source: https://huggingface.co/learn/nlp-course/chapter1/4\n",
            "--------------------------------------------------\n",
            "Text:\n",
            "transfer learning\n",
            ".\n",
            " \n",
            " \n",
            " \n",
            "Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.\n",
            " \n",
            "This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model — one as close as possible to the task you have at hand — and fine-tune it.\n",
            " \n",
            " \n",
            "General architecture\n",
            " \n",
            "In this section, we’ll go over the general architecture of the Transformer model. Don’t worry if you don’t understand some of the concepts; there are detailed sections later covering each of the components.\n",
            " \n",
            " \n",
            " \n",
            "Introduction\n",
            " \n",
            "The model is primarily composed of two blocks:\n",
            " \n",
            "Encoder (left)\n",
            ": The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
            " \n",
            "Decoder (right)\n",
            ": The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.\n",
            " \n",
            " \n",
            " \n",
            "Each of these parts can be used independently, depending on the task:\n",
            " \n",
            "Encoder-only models\n",
            ": Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
            " \n",
            "Decoder-only models\n",
            ": Good for generative tasks such as text generation.\n",
            " \n",
            "Encoder-decoder models\n",
            " or \n",
            "sequence-to-sequence models\n",
            ": Good for generative tasks that require an input, such as translation or summarization.\n",
            " \n",
            "We will dive into those architectures independently in later sections.\n",
            " \n",
            " \n",
            "Attention layers\n",
            " \n",
            "A key feature of Transformer models is that they are built with special layers called \n",
            "attention layers\n",
            ". In fact, the title of the paper introducing the Transformer architecture was \n",
            "“Attention Is All You Need”\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "answer, context = get_answer(\"What are the transformer components?\")\n",
        "get_formatted_answer(answer, context)"
      ],
      "metadata": {
        "trusted": true,
        "id": "nCGAEaMJJIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "799f4ff9-1c0c-4334-c9e2-6b7a41f3cbc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "    Based on the provided context, the components of a Transformer model include:\n",
            "\n",
            "1. Encoder: The encoder receives an input and builds a representation of it (its features).\n",
            "2. Decoder: The decoder uses the encoder's representation along with other inputs to generate a target sequence.\n",
            "3. Attention layers: A key feature of Transformer models that allows them to focus on specific parts of the input when generating output.\n",
            "\n",
            "Additionally, there are three types of Transformer architectures:\n",
            "\n",
            "1. Encoder-only models\n",
            "2. Decoder-only models\n",
            "3. Encoder-decoder models (or sequence-to-sequence models)\n",
            "    ==================================================\n",
            "    \n",
            "Document 1\n",
            "Source: https://huggingface.co/learn/nlp-course/chapter1/3\n",
            "--------------------------------------------------\n",
            "Text:\n",
            "Transformers, what can they do?\n",
            " \n",
            " \n",
            " \n",
            " \n",
            "In this section, we will look at what Transformer models can do and use our first tool from the 🤗 Transformers library: the \n",
            "pipeline()\n",
            " function.\n",
            " \n",
            "👀 See that \n",
            "Open in Colab\n",
            " button on the top right? Click on it to open a Google Colab notebook with all the code samples of this section. This button will be present in any section containing code examples. \n",
            "If you want to run the examples locally, we recommend taking a look at the \n",
            "setup\n",
            ".\n",
            " \n",
            " \n",
            "Transformers are everywhere!\n",
            " \n",
            "Transformer models are used to solve all kinds of NLP tasks, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:\n",
            " \n",
            " \n",
            "The \n",
            "🤗 Transformers library\n",
            " provides the functionality to create and use those shared models. The \n",
            "Model Hub\n",
            " contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub!\n",
            " \n",
            "⚠️ The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! \n",
            "Create a huggingface.co\n",
            " account to benefit from all available features!\n",
            " \n",
            "Before diving into how Transformer models work under the hood, let’s look at a few examples of how they can be used to solve some interesting NLP problems.\n",
            " \n",
            " \n",
            "Working with pipelines\n",
            " \n",
            " \n",
            "The most basic object in the 🤗 Transformers library is the \n",
            "pipeline()\n",
            " function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:\n",
            " \n",
            " \n",
            " Copied\n",
            " \n",
            "from\n",
            " transformers \n",
            "import\n",
            " pipeline\n",
            "==================================================\n",
            "Document 2\n",
            "Source: https://huggingface.co/learn/nlp-course/chapter1/4\n",
            "--------------------------------------------------\n",
            "Text:\n",
            "transfer learning\n",
            ".\n",
            " \n",
            " \n",
            " \n",
            "Fine-tuning a model therefore has lower time, data, financial, and environmental costs. It is also quicker and easier to iterate over different fine-tuning schemes, as the training is less constraining than a full pretraining.\n",
            " \n",
            "This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model — one as close as possible to the task you have at hand — and fine-tune it.\n",
            " \n",
            " \n",
            "General architecture\n",
            " \n",
            "In this section, we’ll go over the general architecture of the Transformer model. Don’t worry if you don’t understand some of the concepts; there are detailed sections later covering each of the components.\n",
            " \n",
            " \n",
            " \n",
            "Introduction\n",
            " \n",
            "The model is primarily composed of two blocks:\n",
            " \n",
            "Encoder (left)\n",
            ": The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.\n",
            " \n",
            "Decoder (right)\n",
            ": The decoder uses the encoder’s representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.\n",
            " \n",
            " \n",
            " \n",
            "Each of these parts can be used independently, depending on the task:\n",
            " \n",
            "Encoder-only models\n",
            ": Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.\n",
            " \n",
            "Decoder-only models\n",
            ": Good for generative tasks such as text generation.\n",
            " \n",
            "Encoder-decoder models\n",
            " or \n",
            "sequence-to-sequence models\n",
            ": Good for generative tasks that require an input, such as translation or summarization.\n",
            " \n",
            "We will dive into those architectures independently in later sections.\n",
            " \n",
            " \n",
            "Attention layers\n",
            " \n",
            "A key feature of Transformer models is that they are built with special layers called \n",
            "attention layers\n",
            ". In fact, the title of the paper introducing the Transformer architecture was \n",
            "“Attention Is All You Need”\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "And another questions that is irrelevant:"
      ],
      "metadata": {
        "id": "BVETCwSWJIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer, context = get_answer(\"What are cookies?\")\n",
        "get_formatted_answer(answer, context)"
      ],
      "metadata": {
        "trusted": true,
        "id": "WEyzc0cwJIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8830be0a-e1bf-4779-eab8-0c55c6bb594e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer:\n",
            "    I don't know based on the provided context. The text appears to be about tokenization and reinforcement learning, but there's no mention of cookies.\n",
            "    ==================================================\n",
            "    \n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "There are various ways to build a Retrieval-Augmented Generation (RAG) system, typically involving three main stages: indexing, retrieval, and generation. Each stage offers multiple implementation options.  \n",
        "\n",
        "- **Indexing**: You can experiment with different vector store types, similarity measures, chunking strategies, and embedding models.  \n",
        "- **Generation**: You might choose between various LLM architectures or explore advanced setups like Agentic RAG, where multiple agents collaborate to generate a better response.  \n",
        "\n",
        "However, to determine which combination works best for your specific use case, a solid evaluation strategy is essential.\n",
        "\n",
        "## Evaluation Approaches\n",
        "\n",
        "There are several ways to evaluate the performance of a RAG system:\n",
        "\n",
        "- **Human Evaluation**: The most reliable method involves using human-labeled datasets for benchmarking. While this provides high-quality feedback, it's often impractical for early-stage projects or quick proofs of concept due to time and cost constraints.\n",
        "\n",
        "- **LLM-as-a-Judge**: A more scalable, though still evolving, approach is to use large language models to evaluate the quality of responses. This method is gaining popularity, especially in rapid prototyping scenarios.\n",
        "\n",
        "In this tutorial, we’ll focus on automatic evaluation techniques based on key quality metrics, using two popular libraries:\n",
        "\n",
        "- [**TruLens**](https://www.trulens.org/)\n",
        "- [**Ragas**](https://github.com/explodinggradients/ragas)\n",
        "\n",
        "These tools help us assess critical aspects of RAG performance such as faithfulness, context relevance, and answer relevance.\n",
        "\n",
        "\n",
        "\n",
        "## TruLens\n",
        "\n",
        "TruLens offers three core metrics to evaluate a RAG system:\n",
        "\n",
        "- **Context Relevance**: Assesses whether the retrieved context is actually relevant to the user's query, helping reduce irrelevant or distracting information.\n",
        "  \n",
        "- **Groundedness**: Measures how well the generated answer sticks to the retrieved context, ensuring the model doesn’t \"hallucinate\" unsupported facts.\n",
        "  \n",
        "- **Answer Relevance**: Evaluates whether the final response meaningfully addresses the user's original query, focusing on usefulness and clarity."
      ],
      "metadata": {
        "id": "zu3LSvhmJIk3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.trulens.org/getting_started/core_concepts/rag_triad/\n",
        "\n",
        "Note that the Trulens chain requires the rag to output only the text answer so we are going to use the chain that has only text values."
      ],
      "metadata": {
        "id": "7o6VeuK2JIk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.apps.langchain import TruChain\n",
        "from trulens.core import TruSession\n",
        "\n",
        "session = TruSession()\n",
        "session.reset_database()"
      ],
      "metadata": {
        "trusted": true,
        "id": "eWovRXGPJIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a7f3521-5b3e-4d84-80ef-d62dea8aef96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🦑 Initialized with db url sqlite:///default.sqlite .\n",
            "🛑 Secret keys may be written to the database. See the `database_redact_keys` option of `TruSession` to prevent this.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Updating app_name and app_version in apps table: 0it [00:00, ?it/s]\n",
            "Updating app_id in records table: 0it [00:00, ?it/s]\n",
            "Updating app_json in apps table: 0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import litellm\n",
        "from trulens.providers.litellm import LiteLLM\n",
        "from trulens.core import Feedback\n",
        "\n",
        "litellm.set_verbose = False\n",
        "\n",
        "# first we need to load the LLM judge fromm ollama\n",
        "ollama_provider = LiteLLM(\n",
        "    model_engine=\"ollama/llama3.1:8b-instruct-q8_0\", api_base=\"http://localhost:11434\"\n",
        ")\n",
        "\n",
        "context = TruChain.select_context(rag_chain_only_str_answer)\n",
        "\n",
        "# now we define our metrics, and the stage of evaluation, for example the groundness will be checked after\n",
        "# after answer generation (getting the answer) and after context generation (getting the context)\n",
        "f_groundedness = (\n",
        "    Feedback(\n",
        "        ollama_provider.groundedness_measure_with_cot_reasons, name=\"Groundedness\"\n",
        "    )\n",
        "    .on(context.collect())  # collect context chunks into a list\n",
        "    .on_output()\n",
        ")\n",
        "\n",
        "f_answer_relevance = Feedback(\n",
        "    ollama_provider.relevance_with_cot_reasons, name=\"Answer Relevance\"\n",
        ").on_input_output()\n",
        "\n",
        "f_context_relevance = (\n",
        "    Feedback(\n",
        "        ollama_provider.context_relevance_with_cot_reasons, name=\"Context Relevance\"\n",
        "    )\n",
        "    .on_input()\n",
        "    .on(context)\n",
        "    .aggregate(np.mean)\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "id": "f3vj4Ke2JIk3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfbfbb5-297a-44ad-d219-67db447cc409"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ In Groundedness, input source will be set to __record__.app.first.steps__.context.first.invoke.rets[:].page_content.collect() .\n",
            "✅ In Groundedness, input statement will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "✅ In Answer Relevance, input prompt will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Answer Relevance, input response will be set to __record__.main_output or `Select.RecordOutput` .\n",
            "✅ In Context Relevance, input question will be set to __record__.main_input or `Select.RecordInput` .\n",
            "✅ In Context Relevance, input context will be set to __record__.app.first.steps__.context.first.invoke.rets[:].page_content .\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from trulens.apps.app import TruApp\n",
        "\n",
        "tru_recorder = TruChain(\n",
        "    rag_chain_only_str_answer,\n",
        "    app_name=\"ChatApplication\",\n",
        "    app_version=\"Chain1\",\n",
        "    feedbacks=[f_groundedness, f_answer_relevance, f_context_relevance],\n",
        ")\n",
        "\n",
        "clear_output()"
      ],
      "metadata": {
        "trusted": true,
        "id": "SlpHCI7cJIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's have some questions that are related to the course that we scrapped that the rag can answer to calculate the metrics."
      ],
      "metadata": {
        "id": "7gSzXrEFJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_lst_tru = [\"What is a transformers?\",\n",
        "         \"What are the transformer components?\",\n",
        "         \"What is the Encoder?\",\n",
        "         \"Explain the transformer work theory\",\n",
        "         \"What is self-Attention\",\n",
        "         \"What is the bias of transformers?\",\n",
        "         \"What are transformer limits?\"\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "id": "TW8ftwH_JIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "final step is to record the metrics as we invoke the model on each question"
      ],
      "metadata": {
        "id": "IbATGtLuJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with tru_recorder as recording:\n",
        "    for question in q_lst_tru:\n",
        "        llm_response = rag_chain.invoke({\"input\": question})[\"answer\"]\n",
        "        print(llm_response)\n",
        "        print(\"=\" * 50)"
      ],
      "metadata": {
        "trusted": true,
        "id": "C0tnG54_JIk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e77ad09-9b79-4a87-d818-7b42c97f2b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers are models used to solve various NLP tasks, such as those mentioned in the previous section. They are provided by the Hugging Face library and can be used for a wide range of applications, including text classification, language translation, sentiment analysis, and more. The 🤗 Transformers library allows users to create and use these shared models, which can be downloaded from the Model Hub or uploaded by users themselves.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the components of a Transformer model include:\n",
            "\n",
            "1. Encoder: The encoder receives an input and builds a representation of it (its features).\n",
            "2. Decoder: The decoder uses the encoder's representation along with other inputs to generate a target sequence.\n",
            "3. Attention layers: A key feature of Transformer models that allows them to focus on specific parts of the input when generating output.\n",
            "\n",
            "Additionally, there are three types of Transformer architectures:\n",
            "\n",
            "1. Encoder-only models\n",
            "2. Decoder-only models\n",
            "3. Encoder-decoder models (or sequence-to-sequence models)\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided CONTEXT, an Encoder model is described as using only the encoder of a Transformer model and is often characterized as having \"bi-directional\" attention. It's best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition, and extractive question answering.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, I'll explain the Transformer work theory.\n",
            "\n",
            "The Transformer model is primarily composed of two blocks: the Encoder and the Decoder. The Encoder receives an input and builds a representation of it (its features), while the Decoder uses the encoder's representation along with other inputs to generate a target sequence.\n",
            "\n",
            "A key feature of Transformer models is that they are built with special layers called Attention Layers, which allow the model to focus on specific parts of the input when generating output. This is in contrast to traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), which process input sequentially or locally.\n",
            "\n",
            "The Transformer architecture can be used for various NLP tasks, including:\n",
            "\n",
            "* Encoder-only models: sentence classification, named entity recognition\n",
            "* Decoder-only models: text generation\n",
            "* Encoder-decoder models (sequence-to-sequence models): translation, summarization\n",
            "\n",
            "This is a high-level overview of the Transformer work theory based on the provided context.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, self-Attention is not explicitly mentioned. However, attention layers are discussed in relation to the Transformer architecture.\n",
            "\n",
            "However, I can infer that self-Attention might be related to the concept of \"bi-directional\" attention mentioned in the context of encoder models, which allows the attention layers to access all the words in the initial sentence. But this is not a direct answer to your question.\n",
            "\n",
            "If you're asking about self-Attention specifically, I don't have enough information to provide an accurate answer based on the provided context.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know based on the provided context. The question about the bias of transformers is not mentioned in the given text, which only discusses the size and environmental impact of transformer models, as well as their capabilities and usage.\n",
            "==================================================\n",
            "I don't know based on the provided context.\n",
            "==================================================\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we can check the dashboard below."
      ],
      "metadata": {
        "id": "0fi-flyaJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "session.get_leaderboard()"
      ],
      "metadata": {
        "trusted": true,
        "id": "F7ulawAOJIk4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "26e0f6b7-9e67-463a-b87d-ad336dbc6d39"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Answer Relevance  Context Relevance  \\\n",
              "app_name        app_version                                        \n",
              "ChatApplication Chain1               0.176471           0.547619   \n",
              "\n",
              "                             Groundedness   latency  total_cost  \n",
              "app_name        app_version                                      \n",
              "ChatApplication Chain1           0.461438  5.679086         0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c860b17b-e2dc-426e-b72c-d5acf2d1c695\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Answer Relevance</th>\n",
              "      <th>Context Relevance</th>\n",
              "      <th>Groundedness</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>app_name</th>\n",
              "      <th>app_version</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ChatApplication</th>\n",
              "      <th>Chain1</th>\n",
              "      <td>0.176471</td>\n",
              "      <td>0.547619</td>\n",
              "      <td>0.461438</td>\n",
              "      <td>5.679086</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c860b17b-e2dc-426e-b72c-d5acf2d1c695')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c860b17b-e2dc-426e-b72c-d5acf2d1c695 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c860b17b-e2dc-426e-b72c-d5acf2d1c695');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"session\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Answer Relevance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.17647058823529413,\n        \"max\": 0.17647058823529413,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.17647058823529413\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Context Relevance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.5476190476190476,\n        \"max\": 0.5476190476190476,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.5476190476190476\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Groundedness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.461437908496732,\n        \"max\": 0.461437908496732,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.461437908496732\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 5.679086147058824,\n        \"max\": 5.679086147058824,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5.679086147058824\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_cost\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see the answers are not bad in many cases, the context relevance can still be improved but the groundness is the best so far."
      ],
      "metadata": {
        "id": "sSFFBb7HJIk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ragas\n",
        "\n",
        "Another valuable library for evaluating LLM-based RAG systems is **Ragas**. It enables you to assess the quality of your system by comparing its outputs against **expected (reference) answers**. These reference answers are typically written by humans, but Ragas also supports generating them using an LLM — a feature we won’t use in this tutorial to maintain control over the evaluation process.\n",
        "\n",
        "> _Note: The reference answers used here were generated using GitHub Copilot, based on the same course materials that the RAG system uses._\n",
        "\n",
        "Ragas uses a separate LLM to act as a judge, scoring your system’s responses across several key dimensions, such as faithfulness and relevance. This approach allows for a more nuanced and automated evaluation compared to traditional manual reviews."
      ],
      "metadata": {
        "id": "otvmj0bpJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_lst = [\"What are transformers?\",\n",
        "         \"What are the transformer components?\",\n",
        "         \"What is the Encoder?\"\n",
        "]\n",
        "\n",
        "expected_responses = [\n",
        "    \"Transformers are powerful deep learning models designed for various natural language processing tasks. They can perform sentiment analysis, text generation, summarization, translation, and even answer questions based on context. Hugging Face provides a vast collection of pretrained Transformer models that can be easily accessed using the pipeline() function.\",\n",
        "    \"Transformers consist of two main components: the encoder, which processes and understands the input, and the decoder, which generates the output based on the encoder's representation. A key feature of transformers is the attention layers, which allow the model to focus on relevant parts of the input for better performance. These components work together to handle tasks like translation, summarization, and text generation.\",\n",
        "    \"The encoder is a key component of transformer models, designed to process and build a meaningful representation of the input data (like text). It extracts features from the input, which are then optimized for understanding and interpretation. This representation is later used by the decoder or directly for tasks like classification and named entity recognition.\",\n",
        "]"
      ],
      "metadata": {
        "trusted": true,
        "id": "aqqRUBK_JIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def construct_eval_dataset(q_lst, expected_responses, chain):\n",
        "  dataset = []\n",
        "\n",
        "  for query, reference in zip(q_lst, expected_responses):\n",
        "      relevant_docs = rag_chain.invoke({\"input\": query})[\"context\"]\n",
        "      response = rag_chain.invoke({\"input\": query})[\"answer\"]\n",
        "      dataset.append(\n",
        "          {\n",
        "              \"user_input\": query,\n",
        "              \"retrieved_contexts\": [rdoc.page_content for rdoc in relevant_docs],\n",
        "              \"response\": response,\n",
        "              \"reference\": reference,\n",
        "          }\n",
        "      )\n",
        "\n",
        "  evaluation_dataset = EvaluationDataset.from_list(dataset)\n",
        "\n",
        "  return evaluation_dataset, dataset"
      ],
      "metadata": {
        "trusted": true,
        "id": "bWnf8MVVJIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset, dataset = construct_eval_dataset(q_lst, expected_responses, rag_chain)"
      ],
      "metadata": {
        "id": "40LugXzmoR2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in dataset:\n",
        "  print(data['user_input'])\n",
        "  print(data['response'])\n",
        "  print(data['reference'])\n",
        "  print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7hJUHKtCcYH",
        "outputId": "1b287584-876e-4bc1-ffdb-855c3410b201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are transformers?\n",
            "Transformers are models used to solve various NLP tasks, such as text classification, language translation, and sentiment analysis. They can be used for a wide range of applications, including but not limited to:\n",
            "\n",
            "* Text generation\n",
            "* Sentiment analysis\n",
            "* Language translation\n",
            "* Text summarization\n",
            "* Question answering\n",
            "\n",
            "The 🤗 Transformers library provides the functionality to create and use these models, allowing users to download and utilize thousands of pre-trained models from the Model Hub.\n",
            "Transformers are powerful deep learning models designed for various natural language processing tasks. They can perform sentiment analysis, text generation, summarization, translation, and even answer questions based on context. Hugging Face provides a vast collection of pretrained Transformer models that can be easily accessed using the pipeline() function.\n",
            "==================================================\n",
            "What are the transformer components?\n",
            "Based on the provided context, the components of a Transformer model include:\n",
            "\n",
            "1. Encoder: The encoder receives an input and builds a representation of it (its features).\n",
            "2. Decoder: The decoder uses the encoder's representation along with other inputs to generate a target sequence.\n",
            "3. Attention layers: A key feature of Transformer models that allows them to focus on specific parts of the input when generating output.\n",
            "\n",
            "Additionally, there are three types of Transformer architectures:\n",
            "\n",
            "1. Encoder-only models\n",
            "2. Decoder-only models\n",
            "3. Encoder-decoder models (or sequence-to-sequence models)\n",
            "Transformers consist of two main components: the encoder, which processes and understands the input, and the decoder, which generates the output based on the encoder's representation. A key feature of transformers is the attention layers, which allow the model to focus on relevant parts of the input for better performance. These components work together to handle tasks like translation, summarization, and text generation.\n",
            "==================================================\n",
            "What is the Encoder?\n",
            "Based on the provided CONTEXT, an Encoder model is described as using only the encoder of a Transformer model and is often characterized as having \"bi-directional\" attention. It's best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition, and extractive question answering.\n",
            "The encoder is a key component of transformer models, designed to process and build a meaningful representation of the input data (like text). It extracts features from the input, which are then optimized for understanding and interpretation. This representation is later used by the decoder or directly for tasks like classification and named entity recognition.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_config = RunConfig(timeout=5000)"
      ],
      "metadata": {
        "trusted": true,
        "id": "796BO_1XJIk4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator_llm = LangchainLLMWrapper(llm, run_config)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
        "    llm=evaluator_llm,\n",
        ")\n",
        "\n",
        "result"
      ],
      "metadata": {
        "trusted": true,
        "id": "zw7gsCzEJIk4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e522f1fb0a5e4d7b99c4cbc37c5f8e75",
            "8533668f8c7d4741af87613b2ca5a096",
            "2e6c8fff1f33429194df9877dc7471dc",
            "2afb2edbace646c980c23f3c6db0538a",
            "e79eab4767184a669e17524454aa5e6d",
            "d80fd5c245fd44c9a4c23ea9ea61c852",
            "64f507aaf3174d1e9ed71df01d134a78",
            "3cc74c844efe4c20bac50550fb2d50a2",
            "491d816f53334947ba2c9ffdc4dda402",
            "b319c7c6288c4823b6d321417d115bcf",
            "3b9612fedf474dfeb17ccb417970a8db"
          ]
        },
        "outputId": "932f9031-afc3-4ef2-8b0c-8656af372534"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e522f1fb0a5e4d7b99c4cbc37c5f8e75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context_recall': 0.7778, 'faithfulness': 0.8889, 'factual_correctness(mode=f1)': 0.6067}"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we know how to evaluate it, let's try again but this time with a different, more powerful embedding model."
      ],
      "metadata": {
        "id": "5pdT5-aEJIk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lm = ChatOllama(\n",
        "    model='llama3.3',\n",
        "    temperature=0)\n",
        "\n",
        "vector_store = FAISS.from_documents(splits, embedding_model)\n",
        "\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
        "rag_chain_improved, rag_chain_only_str_answer = construct_rag_chain(llm, prompt, retriever)"
      ],
      "metadata": {
        "trusted": true,
        "id": "yqJpZfhgJIk5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation_dataset, dataset = construct_eval_dataset(q_lst, expected_responses, rag_chain)\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(llm, run_config)\n",
        "\n",
        "result = evaluate(\n",
        "    dataset=evaluation_dataset,\n",
        "    metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
        "    llm=evaluator_llm,\n",
        ")\n",
        "\n",
        "result"
      ],
      "metadata": {
        "trusted": true,
        "id": "lAZKczZxJIk5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "375e0ce03699491699c301de71838ab3",
            "00672e8026aa441e8e5f7e48d23beb60",
            "498ea67d175149708c16a4364da4a307",
            "02f443a38c984c46bab4d7796e0a47fe",
            "042070bc87b64e0cbf495ea3044fa00b",
            "0bc3b0ca28914bd782264ddb2e37eee4",
            "525bed5711f544a8b83fed41a3647b89",
            "3a5445d252904faba185e3cbb2ac6c24",
            "9d7f6bd6568f4b3ebf993b99dbd2e306",
            "a37a10618f1b47b89a48d069569fc19a",
            "52eb81afa655453cae56c75c2010ea40"
          ]
        },
        "outputId": "298d622c-2d0e-4ee5-bb55-63f1c50fc955"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "375e0ce03699491699c301de71838ab3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context_recall': 0.7778, 'faithfulness': 0.8182, 'factual_correctness(mode=f1)': 0.7400}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "with tru_recorder as recording:\n",
        "    for question in q_lst_tru:\n",
        "        llm_response = rag_chain_improved.invoke({\"input\": question})[\"answer\"]\n",
        "        print(llm_response)\n",
        "        print(\"=\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G_HNKCgrKqq",
        "outputId": "4224a40b-74de-468b-c25e-b3f327993a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers are models used to solve various NLP tasks, such as those mentioned in the previous section. They are provided by the Hugging Face library and can be used for a wide range of applications, including text classification, language translation, sentiment analysis, and more. The 🤗 Transformers library allows users to create and use these shared models, which can be downloaded from the Model Hub or uploaded by users themselves.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, the components of a Transformer model include:\n",
            "\n",
            "1. Encoder: The encoder receives an input and builds a representation of it (its features).\n",
            "2. Decoder: The decoder uses the encoder's representation along with other inputs to generate a target sequence.\n",
            "3. Attention layers: A key feature of Transformer models that allows them to focus on specific parts of the input when generating output.\n",
            "\n",
            "Additionally, there are three types of Transformer architectures:\n",
            "\n",
            "1. Encoder-only models\n",
            "2. Decoder-only models\n",
            "3. Encoder-decoder models (or sequence-to-sequence models)\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided CONTEXT, an Encoder model is described as using only the encoder of a Transformer model and is often characterized as having \"bi-directional\" attention. It's best suited for tasks requiring an understanding of the full sentence, such as sentence classification, named entity recognition, and extractive question answering.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, I'll explain the Transformer work theory.\n",
            "\n",
            "The Transformer model is primarily composed of two blocks: the Encoder and the Decoder. The Encoder receives an input and builds a representation of it (its features), while the Decoder uses the encoder's representation along with other inputs to generate a target sequence.\n",
            "\n",
            "A key feature of Transformer models is that they are built with special layers called Attention Layers, which allow the model to focus on specific parts of the input when generating output. This is in contrast to traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), which process input sequentially or locally.\n",
            "\n",
            "The Transformer architecture can be used for various NLP tasks, including:\n",
            "\n",
            "* Encoder-only models: sentence classification, named entity recognition\n",
            "* Decoder-only models: text generation\n",
            "* Encoder-decoder models (sequence-to-sequence models): translation, summarization\n",
            "\n",
            "This is a high-level overview of the Transformer work theory based on the provided context.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the provided context, self-Attention is not explicitly mentioned. However, attention layers are discussed in relation to the Transformer architecture.\n",
            "\n",
            "However, I can infer that self-Attention might be related to the concept of \"bi-directional\" attention mentioned in the context of encoder models, which allows the attention layers to access all the words in the initial sentence.\n",
            "\n",
            "But if we look at the original text about the decoder, it mentions that the decoder's attention layer will only have access to the words before the word currently being generated. This is often referred to as \"self-attention\" or \"auto-regressive attention\", where the model attends to all previous positions when predicting a new position.\n",
            "\n",
            "So, I'll take a educated guess and say that self-Attention in this context refers to the ability of the decoder's attention layer to attend to all previous words when generating a new word.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know based on the provided context. The bias of transformers is not mentioned in the given text.\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/trulens/feedback/llm_provider.py:1770: UserWarning: Failed to process and remove trivial statements. Proceeding with all statements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know based on the provided context.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "session.get_leaderboard()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "FOKXj1xnyJD2",
        "outputId": "ce7c9f89-01ec-46ee-b957-ad09366f9c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                             Answer Relevance  Context Relevance  \\\n",
              "app_name        app_version                                        \n",
              "ChatApplication Chain1               0.236111           0.433333   \n",
              "\n",
              "                             Groundedness   latency  total_cost  \n",
              "app_name        app_version                                      \n",
              "ChatApplication Chain1           0.487037  5.839576         0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac5a1f80-0c19-4e74-a86b-310f45df6edb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Answer Relevance</th>\n",
              "      <th>Context Relevance</th>\n",
              "      <th>Groundedness</th>\n",
              "      <th>latency</th>\n",
              "      <th>total_cost</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>app_name</th>\n",
              "      <th>app_version</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>ChatApplication</th>\n",
              "      <th>Chain1</th>\n",
              "      <td>0.236111</td>\n",
              "      <td>0.433333</td>\n",
              "      <td>0.487037</td>\n",
              "      <td>5.839576</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac5a1f80-0c19-4e74-a86b-310f45df6edb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac5a1f80-0c19-4e74-a86b-310f45df6edb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac5a1f80-0c19-4e74-a86b-310f45df6edb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"session\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Answer Relevance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.23611111111111108,\n        \"max\": 0.23611111111111108,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.23611111111111108\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Context Relevance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.4333333333333333,\n        \"max\": 0.4333333333333333,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.4333333333333333\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Groundedness\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.487037037037037,\n        \"max\": 0.487037037037037,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.487037037037037\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"latency\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 5.839575750000001,\n        \"max\": 5.839575750000001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          5.839575750000001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_cost\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we explored the integration of a Retrieval-Augmented Generation (RAG) system using Llama 3.1. By combining the generative strengths of Llama with a simple retrieval process, we aimed to enhance the model’s output with relevant and accurate context to improve the model output and make it more reliable. Here are the key takeaways:\n",
        "\n",
        "- **Enhanced Relevance:** Incorporating real-time data retrieval into the generation process improves the accuracy and contextual relevance of the output, reduces halucinations of foundationa models and makes us confident about the sosurces of the data without relying on the training data of the LLM itself.\n",
        "- **Room for Optimization:** While the initial results are promising, future work could focus on refining retrieval strategies and fine-tuning the prompt-engineering techniques to further boost performance. Experimenting with alternative datasets and optimizing query algorithms can also pave the way for more robust implementations.\n",
        "\n",
        "Overall, this notebook demonstrates a constructive step towards building AI systems that are not only generative but also context-aware and fact-guided. The insights gained here is the basic foundation on which you can build to improve your RAG application."
      ],
      "metadata": {
        "id": "CLQMq75eJIk5"
      }
    }
  ]
}